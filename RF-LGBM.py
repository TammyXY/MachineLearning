import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import time
from tqdm import tqdm
import warnings
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

warnings.filterwarnings('ignore')


class SmartModelEnsemble:
    def __init__(self):
        self.models = {}
        self.best_model_for_target = {}
        self.feature_importance = {}

    def create_target_specific_features(self, data, feature_cols):
        """ä¸ºä¸åŒç›®æ ‡å˜é‡åˆ›å»ºä¸“é—¨çš„ç‰¹å¾å·¥ç¨‹"""
        df = data.copy()

        # åŸºç¡€ç‰¹å¾
        base_features = feature_cols.copy()

        # === é’ˆå¯¹å‰4ä¸ªç‰¹å¾ï¼ˆRFè¡¨çŽ°å¥½ï¼‰çš„ç‰¹å¾å·¥ç¨‹ ===
        # è¿™äº›ç‰¹å¾é€šå¸¸æœ‰è¾ƒå¼ºçš„æ—¶åºä¾èµ–å’Œçº¿æ€§å…³ç³»

        # æ»žåŽç‰¹å¾
        for col in feature_cols:
            for lag in [1, 2, 3, 5]:
                df[f'{col}_lag_{lag}'] = df[col].shift(lag)
                base_features.append(f'{col}_lag_{lag}')

        # æ»šåŠ¨ç»Ÿè®¡ - RFæ“…é•¿å¤„ç†è¿™ç±»ç‰¹å¾
        for col in feature_cols:
            for window in [3, 5, 10]:
                df[f'{col}_roll_mean_{window}'] = df[col].rolling(window, min_periods=1).mean()
                df[f'{col}_roll_std_{window}'] = df[col].rolling(window, min_periods=1).std()
                base_features.extend([
                    f'{col}_roll_mean_{window}',
                    f'{col}_roll_std_{window}'
                ])

        # === é’ˆå¯¹åŽ2ä¸ªç‰¹å¾ï¼ˆLGBMè¡¨çŽ°å¥½ï¼‰çš„ç‰¹æ®Šç‰¹å¾ ===
        # è¿™äº›ç‰¹å¾ï¼ˆä¿¡å·å¼ºåº¦ï¼‰é€šå¸¸æœ‰å¤æ‚çš„éžçº¿æ€§å…³ç³»

        # ä¿¡å·å¼ºåº¦ä¸“é—¨ç‰¹å¾
        signal_features = []

        # ä¿¡å·æ¯”çŽ‡å’Œäº¤äº’
        df['H2O_CO2_sig_ratio'] = df['Error_H2O_sig_strgth'] / (df['Error_CO2_sig_strgth'] + 1e-8)
        df['sig_strength_sum'] = df['Error_H2O_sig_strgth'] + df['Error_CO2_sig_strgth']
        signal_features.extend(['H2O_CO2_sig_ratio', 'sig_strength_sum'])

        # ä¿¡å·æ³¢åŠ¨ç‰¹å¾ - LGBMæ“…é•¿å­¦ä¹ è¿™ç±»å¤æ‚æ¨¡å¼
        for col in ['Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']:
            for window in [5, 10, 20]:
                # æ³¢åŠ¨çŽ‡
                df[f'{col}_volatility_{window}'] = df[col].rolling(window).std() / (
                            df[col].rolling(window).mean() + 1e-8)
                # åŠ¨é‡ç‰¹å¾
                df[f'{col}_momentum_{window}'] = df[col] - df[col].shift(window)
                signal_features.extend([
                    f'{col}_volatility_{window}',
                    f'{col}_momentum_{window}'
                ])

        # ä¿¡å·å˜åŒ–çŽ‡
        df['H2O_sig_change_rate'] = df['Error_H2O_sig_strgth'].pct_change()
        df['CO2_sig_change_rate'] = df['Error_CO2_sig_strgth'].pct_change()
        signal_features.extend(['H2O_sig_change_rate', 'CO2_sig_change_rate'])

        # å¡«å……NaNå€¼
        df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)

        # è¿”å›žæ‰€æœ‰ç‰¹å¾
        all_features = base_features + signal_features

        return df, all_features, base_features, signal_features

    def select_best_model_per_target(self, X_train, y_train, X_val, y_val, target_columns):
        """ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡é€‰æ‹©æœ€ä½³æ¨¡åž‹"""
        print("ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡é€‰æ‹©æœ€ä½³æ¨¡åž‹...")

        # æ¨¡åž‹å‚æ•°
        rf_params = {
            'n_estimators': 200,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt',
            'n_jobs': -1,
            'random_state': 217,
            'verbose': 0
        }

        lgb_params = {
            'n_estimators': 300,
            'learning_rate': 0.05,
            'max_depth': 8,
            'num_leaves': 64,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'n_jobs': -1,
            'random_state': 217,
            'verbose': -1
        }

        best_models = {}
        validation_results = {}

        for i, target in enumerate(tqdm(target_columns, desc="æ¨¡åž‹é€‰æ‹©")):
            # åˆ†åˆ«ç”¨RFå’ŒLGBMè®­ç»ƒå¹¶éªŒè¯
            rf_model = RandomForestRegressor(**rf_params)
            lgb_model = LGBMRegressor(**lgb_params)

            rf_model.fit(X_train, y_train[:, i])
            lgb_model.fit(X_train, y_train[:, i])

            rf_pred = rf_model.predict(X_val)
            lgb_pred = lgb_model.predict(X_val)

            # è®¡ç®—éªŒè¯é›†æ€§èƒ½
            rf_r2 = r2_score(y_val[:, i], rf_pred)
            lgb_r2 = r2_score(y_val[:, i], lgb_pred)

            rf_mae = mean_absolute_error(y_val[:, i], rf_pred)
            lgb_mae = mean_absolute_error(y_val[:, i], lgb_pred)

            # é€‰æ‹©æœ€ä½³æ¨¡åž‹
            if rf_r2 > lgb_r2 and rf_mae < lgb_mae:
                best_model = rf_model
                best_model_name = 'RF'
                best_score = rf_r2
            else:
                best_model = lgb_model
                best_model_name = 'LGBM'
                best_score = lgb_r2

            best_models[target] = best_model
            self.best_model_for_target[target] = best_model_name

            validation_results[target] = {
                'RF_R2': rf_r2,
                'LGBM_R2': lgb_r2,
                'RF_MAE': rf_mae,
                'LGBM_MAE': lgb_mae,
                'Best_Model': best_model_name,
                'Best_Score': best_score
            }

            print(f"  {target}: {best_model_name} (RF_R2: {rf_r2:.4f}, LGBM_R2: {lgb_r2:.4f})")

        self.models = best_models
        return validation_results

    def train_final_models(self, X_train, y_train, target_columns, feature_sets=None):
        """ä½¿ç”¨é€‰å®šçš„æœ€ä½³æ¨¡åž‹è¿›è¡Œæœ€ç»ˆè®­ç»ƒ"""
        print("è®­ç»ƒæœ€ç»ˆæ¨¡åž‹...")

        # æ¨¡åž‹å‚æ•°ï¼ˆä½¿ç”¨å®Œæ•´æ•°æ®è®­ç»ƒï¼‰
        rf_final_params = {
            'n_estimators': 300,
            'max_depth': 20,
            'min_samples_split': 3,
            'min_samples_leaf': 1,
            'max_features': 'sqrt',
            'n_jobs': -1,
            'random_state': 217,
            'verbose': 0
        }

        lgb_final_params = {
            'n_estimators': 500,
            'learning_rate': 0.05,
            'max_depth': 10,
            'num_leaves': 128,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'n_jobs': -1,
            'random_state': 217,
            'verbose': -1
        }

        # ä¸ºä¿¡å·å¼ºåº¦ç‰¹å¾ä½¿ç”¨ä¸“é—¨çš„ç‰¹å¾é›†
        if feature_sets:
            signal_targets = ['H2O_sig_strgth', 'CO2_sig_strgth']
            signal_features = feature_sets['signal_features']
            base_features = feature_sets['base_features']
        else:
            signal_targets = []
            signal_features = base_features = X_train.columns.tolist()

        for i, target in enumerate(tqdm(target_columns, desc="æœ€ç»ˆè®­ç»ƒ")):
            # é€‰æ‹©ç‰¹å¾é›†
            if target in signal_targets and feature_sets:
                X_train_selected = X_train[signal_features]
            else:
                X_train_selected = X_train[base_features]

            # æ ¹æ®ä¹‹å‰çš„é€‰æ‹©è®­ç»ƒæœ€ç»ˆæ¨¡åž‹
            if self.best_model_for_target[target] == 'RF':
                model = RandomForestRegressor(**rf_final_params)
            else:
                model = LGBMRegressor(**lgb_final_params)

            model.fit(X_train_selected, y_train[:, i])
            self.models[target] = model

        return self.models

    def predict_with_best_models(self, X_test, target_columns, feature_sets=None):
        """ä½¿ç”¨æœ€ä½³æ¨¡åž‹è¿›è¡Œé¢„æµ‹"""
        print("ä½¿ç”¨æœ€ä½³æ¨¡åž‹è¿›è¡Œé¢„æµ‹...")

        predictions = np.zeros((len(X_test), len(target_columns)))

        for i, target in enumerate(target_columns):
            # é€‰æ‹©ç‰¹å¾é›†
            if target in ['H2O_sig_strgth', 'CO2_sig_strgth'] and feature_sets:
                X_test_selected = X_test[feature_sets['signal_features']]
            else:
                X_test_selected = X_test[feature_sets['base_features']]

            # ä½¿ç”¨å¯¹åº”çš„æœ€ä½³æ¨¡åž‹é¢„æµ‹
            pred = self.models[target].predict(X_test_selected)
            predictions[:, i] = pred

            print(f"  {target}: {self.best_model_for_target[target]}")

        return predictions

    def evaluate_performance(self, y_true, predictions, target_columns):
        """è¯„ä¼°æ€§èƒ½"""
        print("\n" + "=" * 70)
        print("æ™ºèƒ½æ¨¡åž‹é€‰æ‹©æ€§èƒ½è¯„ä¼°")
        print("=" * 70)

        results = {}
        overall_errors = []

        print("\nå„ç‰¹å¾è¯¦ç»†ç»“æžœ:")
        for i, target in enumerate(target_columns):
            r2 = r2_score(y_true[:, i], predictions[:, i])
            mae = mean_absolute_error(y_true[:, i], predictions[:, i])
            rmse = np.sqrt(mean_squared_error(y_true[:, i], predictions[:, i]))

            results[target] = {
                'R2': r2,
                'MAE': mae,
                'RMSE': rmse,
                'Model': self.best_model_for_target[target]
            }

            overall_errors.append(mae)

            print(f"  {target:25} | {self.best_model_for_target[target]:4} | "
                  f"RÂ²: {r2:7.4f} | MAE: {mae:9.6f} | RMSE: {rmse:9.6f}")

        # æ€»ä½“æŒ‡æ ‡
        overall_mae = np.mean(overall_errors)
        overall_r2 = r2_score(y_true, predictions, multioutput='variance_weighted')

        results['overall'] = {
            'MAE': overall_mae,
            'R2': overall_r2
        }

        print(f"\næ€»ä½“æ€§èƒ½: MAE = {overall_mae:.6f}, RÂ² = {overall_r2:.4f}")

        # æ¨¡åž‹ä½¿ç”¨ç»Ÿè®¡
        rf_count = sum(1 for model in self.best_model_for_target.values() if model == 'RF')
        lgb_count = sum(1 for model in self.best_model_for_target.values() if model == 'LGBM')

        print(f"\næ¨¡åž‹ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  Random Forest: {rf_count} ä¸ªç‰¹å¾")
        print(f"  LightGBM:      {lgb_count} ä¸ªç‰¹å¾")

        return results, overall_mae


def main():
    start_time = time.time()

    print("=== æ™ºèƒ½æ¨¡åž‹é€‰æ‹©é›†æˆæ–¹æ¡ˆ ===")
    print("ç­–ç•¥: å‰4ä¸ªç‰¹å¾ç”¨RFï¼ŒåŽ2ä¸ªç‰¹å¾ç”¨LGBM")

    # åŠ è½½æ•°æ®
    print("1. åŠ è½½æ•°æ®...")
    train_data = pd.read_csv(r'C:/ProgramData/anaconda3/envs/pythonProject2/machinelearning/002-æ•°æ®é›†/åŠ å™ªæ•°æ®é›†/modified_æ•°æ®é›†Time_Series661_detail.dat')
    test_data = pd.read_csv(r'C:/ProgramData/anaconda3/envs/pythonProject2/machinelearning/002-æ•°æ®é›†/åŠ å™ªæ•°æ®é›†/modified_æ•°æ®é›†Time_Series662_detail.dat')

    # æ•°æ®é‡‡æ ·ï¼ˆå¯é€‰ï¼‰
    # train_data = train_data.iloc[::2].reset_index(drop=True)
    # test_data = test_data.iloc[::2].reset_index(drop=True)

    # å®šä¹‰ç‰¹å¾å’Œç›®æ ‡
    feature_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr',
                       'Error_H2O_density', 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']
    target_columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr',
                      'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']

    # å‰4ä¸ªç›®æ ‡ï¼ˆRFè¡¨çŽ°å¥½ï¼‰
    rf_targets = target_columns[:4]
    # åŽ2ä¸ªç›®æ ‡ï¼ˆLGBMè¡¨çŽ°å¥½ï¼‰
    lgb_targets = target_columns[4:]

    print(f"RFç›®æ ‡: {rf_targets}")
    print(f"LGBMç›®æ ‡: {lgb_targets}")

    # åˆ›å»ºæ™ºèƒ½é›†æˆæ¨¡åž‹
    smart_ensemble = SmartModelEnsemble()

    # ç‰¹å¾å·¥ç¨‹
    print("2. ç›®æ ‡ç‰¹å®šçš„ç‰¹å¾å·¥ç¨‹...")
    train_enhanced, all_features, base_features, signal_features = smart_ensemble.create_target_specific_features(
        train_data, feature_columns
    )
    test_enhanced, _, _, _ = smart_ensemble.create_target_specific_features(test_data, feature_columns)

    print(f"åŸºç¡€ç‰¹å¾æ•°é‡: {len(base_features)}")
    print(f"ä¿¡å·ç‰¹å¾æ•°é‡: {len(signal_features)}")
    print(f"æ€»ç‰¹å¾æ•°é‡: {len(all_features)}")

    # å‡†å¤‡æ•°æ®
    X_train = train_enhanced[all_features]
    X_test = test_enhanced[all_features]
    y_train = train_data[target_columns].values
    y_test = test_data[target_columns].values

    # åˆ’åˆ†éªŒè¯é›†ï¼ˆç”¨äºŽæ¨¡åž‹é€‰æ‹©ï¼‰
    split_idx = int(0.8 * len(X_train))
    X_tr, X_val = X_train[:split_idx], X_train[split_idx:]
    y_tr, y_val = y_train[:split_idx], y_train[split_idx:]

    print(f"è®­ç»ƒæ•°æ®: X_train{X_tr.shape}, y_train{y_tr.shape}")
    print(f"éªŒè¯æ•°æ®: X_val{X_val.shape}, y_val{y_val.shape}")
    print(f"æµ‹è¯•æ•°æ®: X_test{X_test.shape}, y_test{y_test.shape}")

    # æ¨¡åž‹é€‰æ‹©
    print("3. ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡é€‰æ‹©æœ€ä½³æ¨¡åž‹...")
    validation_results = smart_ensemble.select_best_model_per_target(
        X_tr, y_tr, X_val, y_val, target_columns
    )

    # ä½¿ç”¨å®Œæ•´æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡åž‹
    print("4. ä½¿ç”¨å®Œæ•´æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡åž‹...")
    feature_sets = {
        'base_features': base_features,
        'signal_features': signal_features + base_features  # ä¿¡å·ç‰¹å¾åŒ…å«åŸºç¡€ç‰¹å¾
    }

    smart_ensemble.train_final_models(X_train, y_train, target_columns, feature_sets)

    # é¢„æµ‹
    print("5. é¢„æµ‹...")
    predictions = smart_ensemble.predict_with_best_models(X_test, target_columns, feature_sets)

    # è¯„ä¼°
    print("6. è¯„ä¼°...")
    results, overall_mae = smart_ensemble.evaluate_performance(y_test, predictions, target_columns)

    # ä¿å­˜ç»“æžœ
    print("7. ä¿å­˜ç»“æžœ...")
    results_df = []
    for i in tqdm(range(len(y_test)), desc="ä¿å­˜ç»“æžœ"):
        true_str = ' '.join(map(str, y_test[i]))
        pred_str = ' '.join(map(str, predictions[i]))
        error_str = ' '.join(map(str, np.abs(y_test[i] - predictions[i])))
        results_df.append([true_str, pred_str, error_str])

    result_df = pd.DataFrame(results_df, columns=['True_Value', 'Predicted_Value', 'Error'])
    result_df.to_csv("result_Smart_Model_Selection.csv", index=False)

    # è®¡ç®—å¹³å‡è¯¯å·®
    errors = np.abs(y_test - predictions)
    mean_errors = np.mean(errors, axis=0)

    print("\n" + "=" * 50)
    print("æœ€ç»ˆå¹³å‡è¯¯å·®")
    print("=" * 50)
    for i, col in enumerate(target_columns):
        print(f"{col}: {mean_errors[i]:.6f}")
    print(f"æ€»ä½“å¹³å‡è¯¯å·®: {overall_mae:.6f}")

    # ä¸Žç›®æ ‡å¯¹æ¯”
    target_error = 0.2
    print(f"\nç›®æ ‡è¯¯å·®: {target_error}")
    print(f"å½“å‰è¯¯å·®: {overall_mae:.6f}")
    print(f"å·®è·: {overall_mae - target_error:.6f}")

    if overall_mae <= target_error:
        print(" å·²è¾¾åˆ°ç›®æ ‡è¯¯å·®!")
    else:
        improvement_needed = ((overall_mae - target_error) / target_error) * 100
        print(f"ðŸ“ˆ è¿˜éœ€è¦æ”¹å–„ {improvement_needed:.1f}%")

    end_time = time.time()
    total_time = (end_time - start_time) / 60
    print(f"\næ€»è¿è¡Œæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")


if __name__ == "__main__":
    main()
